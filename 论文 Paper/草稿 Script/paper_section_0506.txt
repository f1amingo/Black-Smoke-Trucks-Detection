2.2	深度卷积神经网络代表
1998年经典的LeNet诞生，它是现代CNN的起点，然而之后CNN的锋芒开始被SVM等手工设计的特征盖过。随着ReLU和dropout的提出，以及GPU和大数据带来的历史机遇，CNN在2012年迎来了历史突破AlexNet。
自从2012年AlexNet在ImageNet（国际著名的计算机视觉竞赛）获得竞赛冠军之后，各种各样更多的基于CNN的深层神经网络模型被提出，并不断刷新着竞赛记录，比如使用重复元素的网络（VGG）、网络中的网络（NiN）、含并行连结的网络（GoogLeNet）、残差网络（ResNet）和稠密连接网络（DenseNet）。
它们中有不少在过去几年的ImageNet比赛中大放异彩，从中我们也可以看出深度卷积神经网络发展的趋势，深度越来越深，可实际上想要获得一个有效的深度模型并不仅仅是加深深度那么简单，网络越深，梯度消失现象就越来越明显，因此出现了批量归一化以及残差网络这样一些方法。
此外，考虑到一些移动设备的应用场景，本文还将介绍mobileNet，其在速度、模型大小上做了优化，并保持精度基本不变。

2.2.1	LeNet
LeNet最早用于手写数字识别，其准确度达到了商用级别。LeNet展示了通过梯度下降训练卷积神经网络可以达到手写数字识别在当时最先进的结果。这个奠基性的工作第一次将卷积神经网络推上舞台，为世人所知。
LeNet分为卷积层块和全连接层块两个部分。卷积层块里的基本单位是卷积层后接最大池化层：卷积层用来识别图像里的空间模式，如线条和物体局部，之后的最大池化层则用来降低卷积层对位置的敏感性。卷积层块由两个这样的基本单位重复堆叠构成。在卷积层块中，每个卷积层都使用 5×5 的窗口，并在输出上使用sigmoid激活函数。第一个卷积层输出通道数为6，第二个卷积层输出通道数则增加到16。这是因为第二个卷积层比第一个卷积层的输入的高和宽要小，所以增加输出通道使两个卷积层的参数尺寸类似。卷积层块的两个最大池化层的窗口形状均为 2×2 ，且步幅为2。由于池化窗口与步幅形状相同，池化窗口在输入上每次滑动所覆盖的区域互不重叠。
卷积层块的输出形状为(批量大小, 通道, 高, 宽)。当卷积层块的输出传入全连接层块时，全连接层块会将小批量中每个样本变平（flatten）。也就是说，全连接层的输入形状将变成二维，其中第一维是小批量中的样本，第二维是每个样本变平后的向量表示，且向量长度为通道、高和宽的乘积。全连接层块含3个全连接层。它们的输出个数分别是120、84和10，其中10为输出的类别个数。
【LeNet图】

2.2.2	AlexNet
LeNet提出后的很长一段时间，CNN一度被其他机器学习方法超越，如支持向量机。虽然LeNet可以在早期的小数据集上取得好的成绩，但是在更大的真实数据集上的表现并不尽如人意。一方面，神经网络计算复杂，没有像GPU这样大量普及的神经网络加速硬件，训练一个多层、多通道、大量参数的卷积神经网络在当时很难完成。另一方面，当时关于参数初始化、非凸优化算法等诸多领域的研究还很缺乏，导致复杂的神经网络的训练通常较困难。
2012年AlexNet使用8层卷积神经网络，以显著的优势赢得了ImageNet 2012图像识别挑战赛，证明了学习到的特征可以超越手工设计的特征，打破了计算机视觉研究的传统观念。
AlexNet包含8层变换，其中有5层卷积和2层全连接隐藏层，以及1个全连接输出层。
AlexNet的基础设计理念沿袭LeNet，此外它还使用了以下一些新技术点：
1.AlexNet使用ReLU替代了sigmoid作为激活函数。由于没有sigmoid激活函数中的求幂运算，ReLU的计算更加简单。此外，ReLU可以使梯度下降更快，模型训练更加容易，因为当sigmoid激活输出趋近于0或1时，这些区域的梯度将会很不明显，几乎为0，从而造成反向传播无法继续更新部分模型参数；而ReLU激活函数在正区间梯度衡为1。
2.训练时使用丢弃法(Dropout)随机忽略一部分神经元，避免过拟合，控制全连接层的模型复杂度。
3.AlexNet使用了大量的图像增广，如裁剪、翻转和颜色变化，极大地扩充了数据集，减轻过拟合，提高模型泛化能力。
4.AlexNet使用重叠的最大池化。最大池化避免了平均池化模糊显著特征的缺点，步长比池化核的尺寸小，使得池化层输出之间会有重叠，提升了特征的丰富性。


2.2.3	VGG
VGG模型获得了2014年ILSVRC竞赛的第二名，第一名是GoogleNet，但是VGG模型在多个迁移学习任务中的表现要优于googleNet。VGG提出了一种构建深度模型的新思路，通过重复使用简单的基础块来构建深度模型。
VGG块的组成规律是：连续使用数个相同的填充为1、窗口形状为 3×3 的卷积层后接上一个步幅为2、窗口形状为 2×2 的最大池化层。卷积层保持输入的高和宽不变，而池化层则对其减半。
【VGG网络结构图】

2.2.4	ResNet
ResNet在2015年被提出，在ImageNet比赛classification任务上获得第一名。
理论上，当我们为神经网络模型添加新的层，充分训练后的模型应该能更有效地降低训练误差。可是实验中却发现，深度网络出现了退化问题(Degradation problem)：网络深度增加，网络准确度出现饱和，甚至出现下降。虽然有如批量归一化(BatchNorm)等一些技术手段来缓解这个问题，该问题仍然存在。ResNet的提出就致力于解决该问题。
ResNet的基本结构是残差块。网络输入是x，网络的输出是F(x)，网络要拟合的目标是H(x)，传统网络的训练目标是F(x)=H(x)。残差网络，则是把传统网络的输出F(x)处理一下，加上输入，将F(x)+x作为最终的输出，训练目标是F(x)=H(x)-x，因此得名残差网络。
现在我们要训练一个很深的网络，假设存在一个最优的完美网络N，与它相比我们的网络中必定有一些层是多余的，那么这些多余的层的训练目标是恒等变换，只有达到这个目标我们的网络性能才能跟N一样。
对于这些需要实现恒等变换的多余的层，要拟合的目标就成了H(x)=x，在传统网络中，网络的输出目标是F(x)=x，这比较困难，而在残差网络中，拟合的目标成了x-x=0，网络的输出目标为F(x)=0，这比前者要容易得多。


2.2.5	mobileNet
随着模型深度越来越深，模型复杂度也越来越高，ResNet甚至可以达到152层的深度，然而在某些真实的应用场景如移动或者嵌入式设备，如此大而复杂的模型是难以被应用的。
MobileNet是由Google最近提出的一种小巧而高效的CNN模型，其在保持模型性能（accuracy）的前提下降低模型大小（parameters size），同时提升模型速度（speed, low latency）。
MobileNet的基本单元是深度级可分离卷积（depthwise separable convolution），可以理解为一种可分解卷积操作（factorized convolutions），其可以分解为两个更小的操作：depthwise convolution和pointwise convolution。Depthwise convolution和标准卷积不同，对于标准卷积其卷积核是用在所有的输入通道上（input channels），而depthwise convolution针对每个输入通道采用不同的卷积核，就是说一个卷积核对应一个输入通道，所以说depthwise convolution是depth级别的操作。而pointwise convolution其实就是普通的卷积，只不过其采用1x1的卷积核。
对于depthwise separable convolution，其首先是采用depthwise convolution对不同输入通道分别进行卷积，然后采用pointwise convolution将上面的输出再进行结合，这样其实整体效果和一个标准卷积是差不多的，但是会大大减少计算量和模型参数量。